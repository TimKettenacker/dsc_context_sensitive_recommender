1+1
23*8
install.packages("DT")
install.packages('plyr')
library(plyr)
install.packages("ggplot2")
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
install.packages("ggthemes")
install.packages("plotly")
install.packages("sp")
install.packages("leaflet")
knitr::opts_chunk$set(echo = FALSE)
raw <- read.csv("dqaudit_testset.csv", sep=";", na.strings = "", colClasses = c("X_Koordinate"="character", "Y_Koordinate"="character", "PLZ"="character", "POST_Postleitzahl"="character", "Ergebnisklasse_der_Zustelladresse"="character"), fileEncoding = "UTF-8")
View(raw)
p <- count(raw$SRC_mail2Stat)
View(p)
p['x']
p["x"]
p["x"] <- c("Singles", "Head duplicates", "duplicates")
View(p)
View(p)
plot_ly(p, labels = x, values = freq, type = "pie")
install.packages('plotly')
require(plotly)
plot_ly(p, labels = x, values = freq, type = "pie")
View(p)
?plot_ly
plot_ly(p, type = "pie")
View(raw)
View(p)
plot_ly(p, type = "pie", x = x, y = freq)
View(raw)
View(p)
p$x
plot_ly(p, labels = p$x, values = freq, type = "pie")
plot_ly(p, labels = p$x, values = p$freq, type = "pie")
raw <- read.csv("dqaudit_testset.csv", sep=";", na.strings = "", colClasses = c("X_Koordinate"="character", "Y_Koordinate"="character", "PLZ"="character", "POST_Postleitzahl"="character", "Ergebnisklasse_der_Zustelladresse"="character"), fileEncoding = "UTF-8")
e_first <- count(is.na(raw$VORNAME))
e_last <- count(is.na(raw$NAME))
e_str <- count(is.na(raw$STRASSE))
e_zip <- count(is.na(raw$PLZ))
e_city <- count(is.na(raw$STADT))
e_first$column <- "first name"
e_last$column <- "last name"
e_str$column <- "street"
e_zip$column <- "zip code"
e_city$column <- "city"
e_fields <- rbind(e_first, e_last, e_str, e_zip, e_city)
empties.df = ddply(e_fields, .(column), transform, percent = freq/sum(freq) * 100)
empties.df = ddply(empties.df, .(column), transform, pos = (cumsum(freq) - 0.5 * freq))
empties.df$label = paste0(sprintf("%.0f", empties.df$percent), "%")
g1 <- ggplot(empties.df, aes(x = column, y = freq, fill = x)) +
geom_bar(stat = "identity", width = .6) + ggtitle("Column filling ratio") +
geom_text(aes(y = pos, label = label), size = 3) +
coord_flip() + theme_solarized_2() + scale_fill_manual(values=c("#8b7765", "#E69F00")) + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())
ggplotly(g1, tooltip = c("x", "y"))
?ggtheme()
?ggthemes()
??ggthemes()
g1 <- ggplot(empties.df, aes(x = column, y = freq, fill = x)) +
geom_bar(stat = "identity", width = .6) + ggtitle("Column filling ratio") +
geom_text(aes(y = pos, label = label), size = 3) +
coord_flip() + theme_solarized() + scale_fill_manual(values=c("#8b7765", "#E69F00")) + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())
ggplotly(g1, tooltip = c("x", "y"))
install.packages('ggthemes')
library(ggthemes)
g1 <- ggplot(empties.df, aes(x = column, y = freq, fill = x)) +
geom_bar(stat = "identity", width = .6) + ggtitle("Column filling ratio") +
geom_text(aes(y = pos, label = label), size = 3) +
coord_flip() + theme_solarized() + scale_fill_manual(values=c("#8b7765", "#E69F00")) + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())
ggplotly(g1, tooltip = c("x", "y"))
require(plotly)
require(ggplot2)
require(ggthemes)
require(plyr)
## make sure read file command contains na.strings parameter to be able to evaluate empty fields
raw <- read.csv("dqaudit_testset.csv", sep=";", na.strings = "", colClasses = c("X_Koordinate"="character", "Y_Koordinate"="character", "PLZ"="character", "POST_Postleitzahl"="character", "Ergebnisklasse_der_Zustelladresse"="character"), fileEncoding = "UTF-8")
e_first <- count(is.na(raw$VORNAME))
e_last <- count(is.na(raw$NAME))
e_str <- count(is.na(raw$STRASSE))
e_zip <- count(is.na(raw$PLZ))
e_city <- count(is.na(raw$STADT))
e_first$column <- "first name"
e_last$column <- "last name"
e_str$column <- "street"
e_zip$column <- "zip code"
e_city$column <- "city"
e_fields <- rbind(e_first, e_last, e_str, e_zip, e_city)
empties.df = ddply(e_fields, .(column), transform, percent = freq/sum(freq) * 100)
empties.df = ddply(empties.df, .(column), transform, pos = (cumsum(freq) - 0.5 * freq))
empties.df$label = paste0(sprintf("%.0f", empties.df$percent), "%")
g1 <- ggplot(empties.df, aes(x = column, y = freq, fill = x)) +
geom_bar(stat = "identity", width = .6) + ggtitle("Column filling ratio") +
geom_text(aes(y = pos, label = label), size = 3) +
coord_flip() + theme_solarized() + scale_fill_manual(values=c("#8b7765", "#E69F00")) + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())
ggplotly(g1, tooltip = c("x", "y"))
g1
ggplotly(g1)
install.packages('UsingR')
install.packages('kernlab')
install.packages('caret')
install.packages('e1071')
install.packages('ISLR')
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~ .,data=training, method = "rf", prox = TRUE)
modFit
?createDataPartition
??createDataPartition
install.packages('caret')
library(caret)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list = FALSE)
install.packages('randomForest')
library("WikidataR")
setwd("/Users/timkettenacker/dsproj_repos/R/dsc_context_sensitive_recommender")
args <- c("Solvay","AM","BI","Proposal")
# the items are already splitted
# remove "Capgemini" if included in the query
args <- args[args != "Capgemini"]
# start conditional enrichment of data
library("WikidataR")
item <- find_item("Solvay", limit = 20)
# checks if any returned item fits a company description
# and returns an integer item list number
check4company <- function(item){
tryCatch(
for(i in 1:length(item)){
if((item[[i]]$description == "company")==TRUE){
relevant_item_no_from_list <- i
}
},error=function(e){}
)
return(relevant_item_no_from_list)
}
item_no <- tryCatch(
check4company(item)
, error=function(e){print("No company name found.
Should be handled more gracefully")})
company_data <- get_item(id = item[[item_no]]$id)
i
get_random_property()
get_property(id = names(item[1]$claims)[1])
item[1]
item[[1]]
t <- get_item(id = 1)
t
get_property(id = names(t$claims)[1])
u <- get_property(id = names(t$claims)[1])
u <- get_property(id = names(t$Claims)[1])
rm(t)
rm(u)
item[1]
??get_property
find_property("peerage")
find_property("Solvay")
get_item(item[[1]]$id)
t <- get_item(item[[1]]$id)
rm(t)
str(company_data)
df <- as.data.frame(company_data)
company_data[[1]]
df <- as.data.frame(company_data[[1]])
str(company_data[[1]])
str(company_data[[1]])$claims
str(company_data[[1]])$aliases
company_data[[1]]$aliases
company_data[[1]]$claims
company_data[[1]]$sitelinks
company_data[[1]]$sitelinks$enwiki$site
company_data[[1]]$sitelinks$enwiki$badges
t <- company_data[[1]]$sitelinks$enwiki$badges
rm(t)
company_data[[1]]$labels
company_data[[1]]$descriptions
str(company_data)
str(company_data[[1]])
company_data[[0]]
company_data[[1]]$id
get_item(id = "Q706184")
company_data[[1]]$sitelinks$enwiki
find_property(company_data[[1]]$sitelinks$enwiki$title)
find_item(company_data[[1]]$sitelinks$enwiki$title)
t <- find_item(company_data[[1]]$sitelinks$enwiki$title)
t[[1]]$url
ompany_data[[1]]$sitelinks$enwiki$title
company_data[[1]]$sitelinks$enwiki$title
library(WikipediR)
t <- page_content(page_name = company_data[[1]]$sitelinks$enwiki$title)
str(company_data[[1]]$sitelinks$enwiki$title)
t <- page_content(page_name = "Solvay S.A.")
??page_content
t <- page_content(language = "en", page_name = "Solvay S.A.")
t <- page_content(language = "en", "wikipedia", page_name = "Solvay S.A.")
t <- page_content(language = "en", "wikipedia", page_name = company_data[[1]]$sitelinks$enwiki$title)
i
i <- 1
company <- find_item(id = item[[i]]$id)
??find_item
??get_item
get_property(id = item[[i]]$id)
get_item(id = item[[i]]$id)
check4company <- function(item){
tryCatch(
for(i in 1:length(item)){
if((item[[i]]$description == "company")==TRUE){
company <- get_property(id = item[[i]]$id)
}
},error=function(e){}
)
return(company)
}
company <- tryCatch(
check4company(item)
, error=function(e){print("No company name found.
Should be handled more gracefully")})
company_data
company_metadata <- tryCatch(
check4company(item)
, error=function(e){print("No company name found.
Should be handled more gracefully")})
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
str(wiki_content)
??page_content
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, clean_response = TRUE)
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE)
str(wiki_content)
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
wiki_content$parse
wiki_content$parse$text$`*`
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE)
wiki_content$parse$text$`*`
wiki_content$parse$wikitext
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
wiki_content$parse$text$`*`
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE)
wiki_content$parse$wikitext
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE, clean_response = TRUE)
wiki_content <- page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
wiki_content[[1]]$text
wiki_content <- tryCatch(
page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
, error=function(e){print("No sitelink found. Should be handled more gracefully.")})
wiki_content
wiki_content$parse$text$`*`[1]
wiki_content$parse$text$`*`[2]
library(xml2)
install.packages("xml2")
library(xml2)
xml_children(wiki_content)
rm(t)
wiki_content <- tryCatch(
page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title)
, error=function(e){print("No sitelink found. Should be handled more gracefully.")})
xml_children(wiki_content)
xml_children(wiki_content$parse$text$`*`)
install.packages("rvest")
company_metadata[[1]]$sitelinks$enwiki$title
wiki_content <- tryCatch(
page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE)
, error=function(e){print("No sitelink found. Should be handled more gracefully.")})
wiki_content$parse$wikitext
str(wiki_content$parse$wikitext)
wiki_content$parse$wikitext$`*`
str(wiki_content$parse$wikitext$`*`)
sub("industry = ", "", wiki_content$parse$wikitext$`*`)
wiki_content_html <- tryCatch(
page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = FALSE)
, error=function(e){print("No sitelink found. Should be handled more gracefully.")})
wiki_content_html$parse$text
str(wiki_content_html$parse$text)
str(wiki_content_html$parse$text$`*`)
company_metadata[[1]]$sitelinks$enwiki$title
page_info(language = "en", project = "wikipedia", domain = NULL, page = company_metadata[[1]]$sitelinks$enwiki$title,
properties = c("protection", "talkid", "url", "displaytitle"),
clean_response = FALSE)
page_info(language = "en", project = "wikipedia", domain = NULL, page = company_metadata[[1]]$sitelinks$enwiki$title,
properties = c("url"),
clean_response = TRUE)
??page_info
url <- tryCatch(
page_info(language = "en", project = "wikipedia", page = company_metadata[[1]]$sitelinks$enwiki$title,
properties = c("url")),
error=function(e){print("No sitelink found. Should be handled more gracefully.")})
url
library(rvest)
html <- read_html("https://en.wikipedia.org/wiki/Solvay_S.A.")
html$node
html_attrs(html$node)
html_attrs(html$doc)
html %>% xml_structure()
html_form(read_html("https://en.wikipedia.org/wiki/Solvay_S.A."))
html_nodes(html, .vcard)
html_nodes(html, ".vcard")
html <- read_html(company_metadata[[1]]$sitelinks$enwiki$title)
company_metadata[[1]]$sitelinks$enwiki$title
url
company_metadata[[1]]$sitelinks
company_metadata[[1]]$sitelinks
company_metadata[[1]]$sitelinks$enwiki
page_metadata <- page_info("en","wikipedia", page = "Aaron Halfaker")
cat("\014")
at("\014")
wiki_content <- tryCatch(
page_content("en", "wikipedia", page_name = company_metadata[[1]]$sitelinks$enwiki$title, as_wikitext = TRUE)
, error=function(e){print("No sitelink found. Should be handled more gracefully.")})
wiki_content$parse$wikitext$`*`
wiki_content_html$parse$text
str(wiki_content_html$parse$text)
html_nodes(wiki_content_html, ".vcard")
html_nodes(wiki_content_html$parse$text, ".vcard")
html_nodes(wiki_content_html$parse$text$`*`, ".vcard")
?sub
str(wiki_content$parse$wikitext$`*`)
?strsplit
strsplit(wiki_content$parse$wikitext$`*`, "n|")
strsplit(wiki_content$parse$wikitext$`*`, "\n|")
strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE)
strsplit(wiki_content_html$parse$text$`*`, "class", fixed = TRUE)
cat("/014")
cat("\014")
parsed_content <- strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE)
parsed_content
?sub
sub("[|[|]|]", "", parsed_content)
parsed_content <- strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE)
parsed_content
parsed_content[4]
parsed_content[1]
parsed_content[[1]]
str(parsed_content)
str(wiki_content)
str(wiki_content$parse$wikitext$`*`)
str(parsed_content[[1]])
str(parsed_content[[20]])
str(parsed_content[[2]])
sub("[|[|]|]", "", wiki_content)
sub("[|[|]|]", "", parsed_content)
sub("[[|]]", "", parsed_content)
sub("[[", "", parsed_content)
sub("[", "", parsed_content)
gsub("[", "", parsed_content)
gsub("[[", "", parsed_content)
cat("/014")
cat("\014")
gsub("\[[", "", parsed_content)
cat("/014")
cat("\014")
gsub("\\[|\\]", "", parsed_content)
gsub("\\[|\\]|\\{|\\}", "", parsed_content)
gsub("\\[|\\]|\\{|\\|\\|}", "", parsed_content)
gsub("\\[|\\]|\\{|\\}", "", parsed_content)
gsub("\\[|\\]|\\{|\\|\\||}", "", parsed_content)
gsub("\\[|\\]|\\{|\\|\\|+}", "", parsed_content)
gsub("\\[|\\]|\\{|\\|\\|+}", "", parsed_content, fixed = TRUE)
gsub("\\[|\\]|\\{|\\|\\|+}", "", parsed_content)
gsub("\\|}", "", parsed_content)
gsub("[a-z]\\|[a-z]}", "", parsed_content)
gsub("\\[|\\]|\\{|\\|\\||}", "", parsed_content)
parsed_content <- strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE) %>% gsub("\\[|\\]|\\{|\\}", "", parsed_content)
parsed_content <- strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE) %>% gsub("\\[|\\]|\\{|\\}", "")
library(margrittr)
library(margritr)
library(magrittr)
wiki_content %>% gsub("\\[|\\]|\\{|\\}", "")
gsub("\\[|\\]|\\{|\\}", "", wiki_content)
parsed_content <- strsplit(wiki_content$parse$wikitext$`*`, "\n|", fixed = TRUE)
parsed_content
parsed_content[[1]][1]
length(parsed_content[[1]])
for(a in 1:length(parsed_content[[1]])){
gsub("\\[|\\]|\\{|\\}", "", parsed_content[[1]][a])
}
parsed_content
for(a in 1:length(parsed_content[[1]])){
parsed_content[[1]][a] <- gsub("\\[|\\]|\\{|\\}", "", parsed_content[[1]][a])
}
parsed_content
